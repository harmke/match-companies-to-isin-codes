{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match company names to ISIN codes using Azure AI Language and Wiki data\n",
    "\n",
    "Entity linking is a natural language processing task that involves identifying and disambiguating entities in a text. Entities are words or phrases that refer to real-world objects, such as people, places, organizations, events, products, etc. Disambiguating entities means resolving the ambiguity that arises when the same word or phrase can refer to different entities, depending on the context. For example, the word \"Apple\" can refer to the fruit, the company, or the Beatles' record label, depending on the text.\n",
    "\n",
    "Entity linking is important for many applications, such as information extraction, knowledge graph construction, question answering, text summarization, and sentiment analysis. By linking entities to their unique identifiers in a knowledge base, such as Wiki data, we can enrich the text with additional information and enable semantic search and reasoning.\n",
    "\n",
    "This example notebook uses the Azure AI Language service and Wiki data to link company names mentioned in unstructured text (e.g. news articles or transcriptions of earning calls) to company ids (ISIN codes). \n",
    "\n",
    "The notebook is divided into the following sections:\n",
    "\n",
    "- Import libraries\n",
    "- Util functions\n",
    "- Get ISIN codes from list of wikipedia pages\n",
    "- Get linked entities from text using Azure AI Language\n",
    "- Function to get ISIN codes from text\n",
    "- Get ISIN codes for each document in `data` folder\n",
    "\n",
    "Please refer to this page for more information on the Azure AI Language entity linking capabilities: https://learn.microsoft.com/en-us/azure/ai-services/language-service/entity-linking/quickstart?tabs=windows&pivots=programming-language-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-textanalytics==5.2.0\n",
      "  Downloading azure_ai_textanalytics-5.2.0-py3-none-any.whl (239 kB)\n",
      "     ---------------------------------------- 0.0/239.3 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 112.6/239.3 kB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 239.3/239.3 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-ai-textanalytics==5.2.0) (1.1.28)\n",
      "Requirement already satisfied: msrest>=0.7.0 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-ai-textanalytics==5.2.0) (0.7.1)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-ai-textanalytics==5.2.0) (1.29.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-ai-textanalytics==5.2.0) (4.12.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.2.0) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.2.0) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from msrest>=0.7.0->azure-ai-textanalytics==5.2.0) (2023.7.22)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from msrest>=0.7.0->azure-ai-textanalytics==5.2.0) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from msrest>=0.7.0->azure-ai-textanalytics==5.2.0) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.2.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.2.0) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.2.0) (3.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.7.0->azure-ai-textanalytics==5.2.0) (3.2.2)\n",
      "Installing collected packages: azure-ai-textanalytics\n",
      "  Attempting uninstall: azure-ai-textanalytics\n",
      "    Found existing installation: azure-ai-textanalytics 5.2.1\n",
      "    Uninstalling azure-ai-textanalytics-5.2.1:\n",
      "      Successfully uninstalled azure-ai-textanalytics-5.2.1\n",
      "Successfully installed azure-ai-textanalytics-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.1.2\n",
      "[notice] To update, run: C:\\Users\\haalkema\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.1/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.3/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.2/1.5 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 9.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\haalkema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.1.2\n",
      "[notice] To update, run: C:\\Users\\haalkema\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\haalkema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install azure-ai-textanalytics==5.2.0\n",
    "%pip install nltk\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import requests\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "language_key = os.environ.get('LANGUAGE_KEY')\n",
    "language_endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isin(wikipedia_name):\n",
    "    redirects = check_redirects(wikipedia_name)\n",
    "    if redirects is not None:\n",
    "        wikipedia_name = redirects\n",
    "    try:\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&sites=enwiki&props=claims&titles={wikipedia_name}&format=json\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "\n",
    "        entity_id = next(iter(data[\"entities\"]))\n",
    "        value = data[\"entities\"][entity_id]['claims']['P946'][0]['mainsnak'][\"datavalue\"][\"value\"]\n",
    "        return value\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def check_redirects(titles):\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&titles={titles}&redirects\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"query\" in data:\n",
    "        if \"redirects\" in data[\"query\"]:\n",
    "            return data[\"query\"][\"redirects\"][0][\"to\"]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ISIN codes from list of wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US0378331005\n",
      "US02079K3059\n",
      "NL0010273215\n",
      "US88160R1014\n",
      "US92343V1044\n"
     ]
    }
   ],
   "source": [
    "companies = [\"Apple_Inc.\", \"Alphabet_Inc.\", \"ASML_Holding\", \"Tesla,_Inc.\", \"Verizon_Communications\"]\n",
    "\n",
    "for company in companies:\n",
    "    print(get_isin(company))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get linked entities from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked Entities:\n",
      "\n",
      "\tName:  Apple Inc. \tId:  Apple Inc. \tUrl:  https://en.wikipedia.org/wiki/Apple_Inc.\n",
      "\tMatches:\n",
      "\t\tText: Apple\n",
      "\t\tConfidence Score: 0.37\n",
      "\t\tText: Apple\n",
      "\t\tConfidence Score: 0.37\n",
      "\tName:  Visi On \tId:  Visi On \tUrl:  https://en.wikipedia.org/wiki/Visi_On\n",
      "\tMatches:\n",
      "\t\tText: Vision\n",
      "\t\tConfidence Score: 0.10\n",
      "\t\tText: Vision\n",
      "\t\tConfidence Score: 0.10\n",
      "\tName:  .pro \tId:  .pro \tUrl:  https://en.wikipedia.org/wiki/.pro\n",
      "\tMatches:\n",
      "\t\tText: Pro\n",
      "\t\tConfidence Score: 0.06\n",
      "\t\tText: Pro\n",
      "\t\tConfidence Score: 0.06\n",
      "\tName:  Walmart \tId:  Walmart \tUrl:  https://en.wikipedia.org/wiki/Walmart\n",
      "\tMatches:\n",
      "\t\tText: Walmart\n",
      "\t\tConfidence Score: 0.30\n",
      "\tName:  Nike, Inc. \tId:  Nike, Inc. \tUrl:  https://en.wikipedia.org/wiki/Nike,_Inc.\n",
      "\tMatches:\n",
      "\t\tText: Nike\n",
      "\t\tConfidence Score: 0.32\n",
      "\tName:  The Vanguard Group \tId:  The Vanguard Group \tUrl:  https://en.wikipedia.org/wiki/The_Vanguard_Group\n",
      "\tMatches:\n",
      "\t\tText: Vanguard\n",
      "\t\tConfidence Score: 0.06\n",
      "\tName:  Stryker \tId:  Stryker \tUrl:  https://en.wikipedia.org/wiki/Stryker\n",
      "\tMatches:\n",
      "\t\tText: Stryker\n",
      "\t\tConfidence Score: 0.09\n",
      "\tName:  Bloomberg L.P. \tId:  Bloomberg L.P. \tUrl:  https://en.wikipedia.org/wiki/Bloomberg_L.P.\n",
      "\tMatches:\n",
      "\t\tText: Bloomberg\n",
      "\t\tConfidence Score: 0.09\n",
      "\tName:  SAP SE \tId:  SAP SE \tUrl:  https://en.wikipedia.org/wiki/SAP_SE\n",
      "\tMatches:\n",
      "\t\tText: SAP\n",
      "\t\tConfidence Score: 0.19\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    documents = [\"\"\"\n",
    "With the upcoming launch of Apple Vision Pro, we're seeing strong excitement in enterprise, leading organizations across many industries, such as Walmart, Nike, Vanguard, Stryker, Bloomberg and SAP started leveraging and investing in Apple Vision Pro as the new platform to bring innovative spatial computing experiences to their customers and employees. \n",
    "\"\"\"]\n",
    "    result = client.recognize_linked_entities(documents = documents)[0]\n",
    "\n",
    "    print(\"Linked Entities:\\n\")\n",
    "    for entity in result.entities:\n",
    "        print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url)\n",
    "        print(\"\\tMatches:\")\n",
    "        for match in entity.matches:\n",
    "            print(\"\\t\\tText:\", match.text)\n",
    "            print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
    "        \n",
    "except Exception as err:\n",
    "    print(\"Encountered exception. {}\".format(err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get ISIN codes from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isin_codes_from_text(client, text):\n",
    "    companies = {}\n",
    "\n",
    "    try:\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            results = client.recognize_linked_entities(documents = [sentence])\n",
    "\n",
    "            for result in results:\n",
    "                for entity in result.entities:\n",
    "                    if entity.data_source_entity_id in companies:\n",
    "                        continue\n",
    "                    isin = get_isin(entity.data_source_entity_id)\n",
    "                    if isin is not None:\n",
    "                        companies.update({entity.data_source_entity_id: isin})\n",
    "                        print(\"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url, \"\\tMatches:\", [match.text for match in entity.matches], \"\\tISIN: \", isin)\n",
    "                        \n",
    "        return companies\n",
    "            \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ISIN codes for each document in data folder\n",
    "\n",
    "Make sure to add any news articles or transcriptions of earning calls to a folder called `data` before running the call below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-chips.txt\n",
      "\tId:  Amazon (company) \tUrl:  https://en.wikipedia.org/wiki/Amazon_(company) \tMatches: ['Amazon', 'AMZN'] \tISIN:  US0231351067\n",
      "\tId:  Nvidia \tUrl:  https://en.wikipedia.org/wiki/Nvidia \tMatches: ['Nvidia'] \tISIN:  US67066G1040\n",
      "tesla.txt\n",
      "\tId:  Tesla, Inc. \tUrl:  https://en.wikipedia.org/wiki/Tesla,_Inc. \tMatches: ['Tesla'] \tISIN:  US88160R1014\n",
      "\tId:  Morningstar, Inc. \tUrl:  https://en.wikipedia.org/wiki/Morningstar,_Inc. \tMatches: ['Morningstar'] \tISIN:  US6177001095\n",
      "\tId:  NIO (car company) \tUrl:  https://en.wikipedia.org/wiki/NIO_(car_company) \tMatches: ['Nio', 'NIO'] \tISIN:  US62914V1061\n",
      "\tId:  Nvidia \tUrl:  https://en.wikipedia.org/wiki/Nvidia \tMatches: ['Nvidia'] \tISIN:  US67066G1040\n",
      "\tId:  Apple Inc. \tUrl:  https://en.wikipedia.org/wiki/Apple_Inc. \tMatches: ['Apple'] \tISIN:  US0378331005\n",
      "\tId:  Microsoft \tUrl:  https://en.wikipedia.org/wiki/Microsoft \tMatches: ['Microsoft'] \tISIN:  US5949181045\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"data\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    with open(file_path, \"r\") as file:\n",
    "        print(filename)\n",
    "        text = file.read()\n",
    "        get_isin_codes_from_text(client, text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
